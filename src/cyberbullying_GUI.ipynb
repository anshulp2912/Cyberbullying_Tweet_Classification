{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cyberbullying_GUI.ipynb","provenance":[],"authorship_tag":"ABX9TyNxUY9TlRoxZej0sYahR3yt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CYBER BULLYING GUI NOTEBOOK\n","\n","---\n","\n","\n","\n","---\n","\n","\n","## NCSU CSC 591: Algorithms for Data Guided Buisness Intelligence\n","#### Contributors: Anmolika Goyal(agoyal4), Anshul Navinbhai Patel(apatel28), Shubhangi Jain(sjain29)\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"5PMc_wtRL-Uo"}},{"cell_type":"markdown","source":["Connect the Google Drive"],"metadata":{"id":"4QYDiy10MD4z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"99DoXIoWLy_f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"70Jt-RqlMQam"}},{"cell_type":"code","source":["# Installing the libraries\n","!pip install emoji==1.6.3\n","!pip install gradio"],"metadata":{"id":"sh9xxsjvMPLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# General Librarires\n","import gradio\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re, string\n","import emoji\n","from tqdm import tqdm\n","# Model Saving\n","import joblib\n","import pickle\n","# Scikit-Learn Functions\n","from sklearn import preprocessing, decomposition, metrics, pipeline\n","from sklearn.model_selection import cross_val_score, train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","# Machine Learning\n","import xgboost as xgb\n","# NLTK\n","import nltk\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('stopwords') \n","nltk.download('punkt')\n","stop_words = stopwords.words('english')\n","# Keras\n","from keras.models import Sequential\n","from keras.layers.recurrent import LSTM, GRU\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.utils import np_utils\n","from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from keras.preprocessing import sequence, text\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import BatchNormalization"],"metadata":{"id":"qF4H_KVNMRie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing the Dataset"],"metadata":{"id":"U0hffwdbMf_S"}},{"cell_type":"code","source":["# Define preprocessing functions\n","\n","# Remove emojis from text\n","def remove_emoji(txt):\n","  txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n","  return txt\n","\n","# Expand common abbreviations\n","def expand_txt(txt):\n","  txt = re.sub(r\"\\'d\", \" would\", txt)\n","  txt = re.sub(r\"\\'ll\", \" will\", txt)\n","  txt = re.sub(r\"can\\'t\", \"can not\", txt)\n","  txt = re.sub(r\"\\'ve\", \" have\", txt)\n","  txt = re.sub(r\"\\'re\", \" are\", txt)\n","  txt = re.sub(r\"\\'s\", \" is\", txt)\n","  txt = re.sub(r\"\\'m\", \" am\", txt)\n","  txt = re.sub(r\"n\\'t\", \" not\", txt)\n","  txt = re.sub(r\"\\'t\", \" not\", txt)\n","  return txt\n","\n","# Remove characters, links, mentions, and punctuations\n","def clean_nonwanted_chars(txt):\n","  # Remove characters\n","  txt = txt.replace('\\n', ' ')\n","  txt = txt.replace('\\r', '')\n","  # Remove mentions and links\n","  txt = re.sub(r'[^\\x00-\\x7f]',r'', txt)\n","  # Remove punctuations\n","  punc_remove = string.punctuation\n","  punc_list = str.maketrans('', '', punc_remove)\n","  txt = txt.translate(punc_list)\n","  txt = [word for word in txt.split() if word not in stop_words]\n","  txt = ' '.join(txt)\n","  return txt\n","\n","# Remove Hashtags\n","def remove_hash(txt):\n","  txt = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', txt)) \n","  txt = \" \".join(word.strip() for word in re.split('#|_', txt))\n","  return txt\n","\n","# Remove characters from between the words\n","def remove_chars(txt):\n","    clean = []\n","    for word in txt.split(' '):\n","        if ('&' in word) | ('$' in word):\n","            clean.append('')\n","        else:\n","            clean.append(word)\n","    txt = ' '.join(clean)\n","    return txt\n","\n","# Remove multiple spaces and tabs\n","def remove_space(txt):\n","  txt = re.sub(\"\\s\\s+\" , \" \", txt)\n","  return txt"],"metadata":{"id":"5Ulu4SZ-MaoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process the textual data\n","def preprocess_text(txt):\n","  txt = txt.lower()\n","  txt = remove_emoji(txt)\n","  txt = expand_txt(txt)\n","  txt = clean_nonwanted_chars(txt)\n","  txt = remove_hash(txt)\n","  txt = remove_chars(txt)\n","  txt = remove_space(txt)\n","  # Stemming the text\n","  tokens = nltk.word_tokenize(txt)\n","  PS = nltk.stem.PorterStemmer()\n","  txt = ' '.join([PS.stem(words) for words in tokens])\n","  return txt"],"metadata":{"id":"k_teeKYNMc37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load all the variables and models\n"],"metadata":{"id":"Im44TNNjNisR"},"execution_count":null,"outputs":[]}]}