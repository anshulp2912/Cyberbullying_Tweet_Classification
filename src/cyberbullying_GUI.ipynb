{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cyberbullying_GUI.ipynb","provenance":[],"authorship_tag":"ABX9TyPiYNEsKuZwD3KlKthNYPtf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CYBER BULLYING GUI NOTEBOOK\n","\n","---\n","\n","## NCSU CSC 591: Algorithms for Data Guided Buisness Intelligence\n","\n","---\n","As social media usage grows across all age groups, the great majority of individuals rely on this crucial medium for day-to-day communication. Because of the pervasiveness of social media, cyberbullying may affect anybody at any time or from any location, and the internet's relative anonymity makes such personal attacks more difficult to stop than conventional bullying.\n","\n","\n","In light of this, this dataset comprises over 47000 tweets labeled with the following cyberbullying categories: Age, Ethnicity, Gender, Religion, Other sort of cyberbullying, Not cyberbullying.\n","\n","Trigger Warning: These tweets either describe a bullying occurrence or are the crime itself; consequently, read them until you are comfortable.\n","\n","---\n","\n","#### Contributors: Anmolika Goyal(agoyal4), Anshul Navinbhai Patel(apatel28), Shubhangi Jain(sjain29)\n","\n","---\n","\n"],"metadata":{"id":"5PMc_wtRL-Uo"}},{"cell_type":"markdown","source":["Connect the Google Drive"],"metadata":{"id":"4QYDiy10MD4z"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"99DoXIoWLy_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650842687375,"user_tz":240,"elapsed":17356,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}},"outputId":"6a6c9372-dfe6-4828-cc04-8849c7aa1dc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"70Jt-RqlMQam"}},{"cell_type":"code","source":["# Installing the libraries\n","!pip install emoji==1.6.3\n","!pip install gradio"],"metadata":{"id":"sh9xxsjvMPLd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650842713138,"user_tz":240,"elapsed":25615,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}},"outputId":"a4d292bd-287a-45d6-daf4-a504bab6b84d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji==1.6.3\n","  Downloading emoji-1.6.3.tar.gz (174 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 5.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=69f87ac1b322d765e5f7a558159fa9f6abfa7ee89b6cc6b81637fd5cccfc0195\n","  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.6.3\n","Collecting gradio\n","  Downloading gradio-2.9.4-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.2 MB/s \n","\u001b[?25hCollecting fastapi\n","  Downloading fastapi-0.75.2-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n","Collecting ffmpy\n","  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n","Collecting analytics-python\n","  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n","Collecting uvicorn\n","  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n","Collecting paramiko\n","  Downloading paramiko-2.10.3-py2.py3-none-any.whl (211 kB)\n","\u001b[K     |████████████████████████████████| 211 kB 27.9 MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n","Collecting markdown-it-py[linkify,plugins]\n","  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n","\u001b[?25hCollecting orjson\n","  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n","\u001b[K     |████████████████████████████████| 253 kB 49.6 MB/s \n","\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n","Collecting pycryptodome\n","  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 38.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n","Collecting python-multipart\n","  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 36.0 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 53.2 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 50.1 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (21.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n","Collecting backoff==1.10.0\n","  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n","Collecting monotonic>=1.5\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n","Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n","  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[K     |████████████████████████████████| 10.9 MB 39.3 MB/s \n","\u001b[?25hCollecting starlette==0.17.1\n","  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 2.9 MB/s \n","\u001b[?25hCollecting anyio<4,>=3.0.0\n","  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n","\u001b[?25hCollecting sniffio>=1.1\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->gradio) (2.0.1)\n","Collecting mdurl~=0.1\n","  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n","Collecting mdit-py-plugins\n","  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[?25hCollecting linkify-it-py~=1.0\n","  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n","Collecting uc-micro-py\n","  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.8)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.1)\n","Collecting cryptography>=2.5\n","  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 35.3 MB/s \n","\u001b[?25hCollecting pynacl>=1.0.1\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[K     |████████████████████████████████| 856 kB 45.9 MB/s \n","\u001b[?25hCollecting bcrypt>=3.1.3\n","  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 547 kB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n","Collecting h11>=0.8\n","  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n","Collecting asgiref>=3.4.0\n","  Downloading asgiref-3.5.0-py3-none-any.whl (22 kB)\n","Building wheels for collected packages: ffmpy, python-multipart\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=14c368f3f93b5a81915daf01a90a936332839afab825fb25177e989184b7dbf8\n","  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n","  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=f37bcacb4ab9e3224f9b3d0d20c0b5040d3a0fcdc2216538048580641b634a72\n","  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n","Successfully built ffmpy python-multipart\n","Installing collected packages: sniffio, mdurl, uc-micro-py, multidict, markdown-it-py, frozenlist, anyio, yarl, starlette, pynacl, pydantic, monotonic, mdit-py-plugins, linkify-it-py, h11, cryptography, bcrypt, backoff, asynctest, async-timeout, asgiref, aiosignal, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, ffmpy, fastapi, analytics-python, aiohttp, gradio\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.5.0 asgiref-3.5.0 async-timeout-4.0.2 asynctest-0.13.0 backoff-1.10.0 bcrypt-3.2.0 cryptography-36.0.2 fastapi-0.75.2 ffmpy-0.3.0 frozenlist-1.3.0 gradio-2.9.4 h11-0.13.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 multidict-6.0.2 orjson-3.6.8 paramiko-2.10.3 pycryptodome-3.14.1 pydantic-1.9.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 sniffio-1.2.0 starlette-0.17.1 uc-micro-py-1.0.1 uvicorn-0.17.6 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["# General Librarires\n","import gradio as gr\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re, string\n","import emoji\n","from tqdm import tqdm\n","# Model Saving\n","import joblib\n","import pickle\n","# Scikit-Learn Functions\n","from sklearn import preprocessing, decomposition, metrics, pipeline\n","from sklearn.model_selection import cross_val_score, train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","# Machine Learning\n","import xgboost as xgb\n","# NLTK\n","import nltk\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('stopwords') \n","nltk.download('punkt')\n","stop_words = stopwords.words('english')\n","# Keras\n","from keras.models import Sequential\n","from keras.layers.recurrent import LSTM, GRU\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.utils import np_utils\n","from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from keras.preprocessing import sequence, text\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import BatchNormalization"],"metadata":{"id":"qF4H_KVNMRie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650842723828,"user_tz":240,"elapsed":10700,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}},"outputId":"5e0661af-b38f-462f-da29-b6cda10ef1a9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["### Preprocessing the Dataset"],"metadata":{"id":"U0hffwdbMf_S"}},{"cell_type":"code","source":["# Define preprocessing functions\n","\n","# Remove emojis from text\n","def remove_emoji(txt):\n","  txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n","  return txt\n","\n","# Expand common abbreviations\n","def expand_txt(txt):\n","  txt = re.sub(r\"\\'d\", \" would\", txt)\n","  txt = re.sub(r\"\\'ll\", \" will\", txt)\n","  txt = re.sub(r\"can\\'t\", \"can not\", txt)\n","  txt = re.sub(r\"\\'ve\", \" have\", txt)\n","  txt = re.sub(r\"\\'re\", \" are\", txt)\n","  txt = re.sub(r\"\\'s\", \" is\", txt)\n","  txt = re.sub(r\"\\'m\", \" am\", txt)\n","  txt = re.sub(r\"n\\'t\", \" not\", txt)\n","  txt = re.sub(r\"\\'t\", \" not\", txt)\n","  return txt\n","\n","# Remove characters, links, mentions, and punctuations\n","def clean_nonwanted_chars(txt):\n","  # Remove characters\n","  txt = txt.replace('\\n', ' ')\n","  txt = txt.replace('\\r', '')\n","  # Remove mentions and links\n","  txt = re.sub(r'[^\\x00-\\x7f]',r'', txt)\n","  # Remove punctuations\n","  punc_remove = string.punctuation\n","  punc_list = str.maketrans('', '', punc_remove)\n","  txt = txt.translate(punc_list)\n","  txt = [word for word in txt.split() if word not in stop_words]\n","  txt = ' '.join(txt)\n","  return txt\n","\n","# Remove Hashtags\n","def remove_hash(txt):\n","  txt = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', txt)) \n","  txt = \" \".join(word.strip() for word in re.split('#|_', txt))\n","  return txt\n","\n","# Remove characters from between the words\n","def remove_chars(txt):\n","    clean = []\n","    for word in txt.split(' '):\n","        if ('&' in word) | ('$' in word):\n","            clean.append('')\n","        else:\n","            clean.append(word)\n","    txt = ' '.join(clean)\n","    return txt\n","\n","# Remove multiple spaces and tabs\n","def remove_space(txt):\n","  txt = re.sub(\"\\s\\s+\" , \" \", txt)\n","  return txt"],"metadata":{"id":"5Ulu4SZ-MaoV","executionInfo":{"status":"ok","timestamp":1650842723829,"user_tz":240,"elapsed":13,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Process the textual data\n","def preprocess_text(txt):\n","  txt = txt.lower()\n","  txt = remove_emoji(txt)\n","  txt = expand_txt(txt)\n","  txt = clean_nonwanted_chars(txt)\n","  txt = remove_hash(txt)\n","  txt = remove_chars(txt)\n","  txt = remove_space(txt)\n","  # Stemming the text\n","  tokens = nltk.word_tokenize(txt)\n","  PS = nltk.stem.PorterStemmer()\n","  txt = ' '.join([PS.stem(words) for words in tokens])\n","  return txt"],"metadata":{"id":"k_teeKYNMc37","executionInfo":{"status":"ok","timestamp":1650842723829,"user_tz":240,"elapsed":11,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Github Models\n","\n","# Load all the variables and models\n","with open(r\"https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/LE.pkl\", \"rb\") as input_file:\n","  LE = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/CountVectorizer.pkl', 'wb') as input_file:\n","  CV = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/TFIDFVectorizer.pkl', 'wb') as input_file:\n","  TFIDF = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/SVD.pkl', 'wb') as input_file:\n","  SVD = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/SVMScaler.pkl', 'wb') as input_file:\n","  SVMScaler = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/embeddings.pkl', 'wb') as input_file:\n","  embeddings = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/NNScaler.pkl', 'wb') as input_file:\n","  NNScaler = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/token.pkl', 'wb') as input_file:\n","  token = pickle.load(input_file)\n","with open('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/word_index.pkl', 'wb') as input_file:\n","  word_index = pickle.load(input_file)\n","\n","model_CV_LR = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_CV_LR.sav')\n","model_TFIDF_LR = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_TFIDF_LR.sav')\n","model_CV_NB = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_CV_NB.sav')\n","model_TFIDF_NB = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_TFIDF_NB.sav')\n","model_CV_XG = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_CV_XG.sav')\n","model_TFIDF_XG = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_TFIDF_XG.sav')\n","model_CV_SVM = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_CV_SVM.sav')\n","model_TFIDF_SVM = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/model_TFIDF_SVM.sav')\n","GLOVE_XB = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/GLOVE_XB.sav')\n","vanillann = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/vanillann.sav')\n","biLSTM = joblib.load('https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/models/biLSTM.sav')\n"],"metadata":{"id":"Im44TNNjNisR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load from Google Drive\n","\n","# Load all the variables and models\n","with open(r\"/content/gdrive/Shareddrives/ADBI_Capstone/models/LE.pkl\", \"rb\") as input_file:\n","  LE = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/CountVectorizer.pkl', 'rb') as input_file:\n","  CV = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/TFIDFVectorizer.pkl', 'rb') as input_file:\n","  TFIDF = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/SVD.pkl', 'rb') as input_file:\n","  SVD = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/SVMScaler.pkl', 'rb') as input_file:\n","  SVMScaler = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/embeddings.pkl', 'rb') as input_file:\n","  embeddings = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/NNScaler.pkl', 'rb') as input_file:\n","  NNScaler = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/token.pkl', 'rb') as input_file:\n","  token = pickle.load(input_file)\n","input_file.close()\n","with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/word_index.pkl', 'rb') as input_file:\n","  word_index = pickle.load(input_file)\n","input_file.close()\n","\n","model_CV_LR = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_LR.sav')\n","model_TFIDF_LR = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_LR.sav')\n","model_CV_NB = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_NB.sav')\n","model_TFIDF_NB = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_NB.sav')\n","model_CV_XG = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_XG.sav')\n","model_TFIDF_XG = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_XG.sav')\n","model_CV_SVM = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_SVM.sav')\n","model_TFIDF_SVM = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_SVM.sav')\n","GLOVE_XB = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/GLOVE_XB.sav')\n","vanillann = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/vanillann.sav')\n","biLSTM = joblib.load('/content/gdrive/Shareddrives/ADBI_Capstone/models/biLSTM.sav')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2lVhLybiVT3","executionInfo":{"status":"ok","timestamp":1650842755982,"user_tz":240,"elapsed":32163,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}},"outputId":"cf614f3f-6270-46e6-8f4c-d7ab84568890"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"code","source":["# Word Vector Functions\n","\n","#Using word_tokenize to create vectors which are normalized for the wole sentence\n","def tokenized_sentence(s):\n","    text = str(s).lower()\n","    #use word_tokenize to split the text into words\n","    text = word_tokenize(text)\n","    #create a list named text which stores the words that are not in stop_words\n","    text = [word for word in text if not word in stop_words]\n","    #check if the word is an alphanumeric\n","    text = [word for word in text if word.isalpha()]\n","    values = []\n","    #for each word in text, append the value of the vector for that word into the values list\n","    for word in text:\n","        try:\n","            values.append(embeddings[word])\n","        except:\n","            continue\n","    values = np.array(values)\n","    vectors = values.sum(axis=0)\n","    if(type(vectors) != np.ndarray):\n","        return np.zeros(300)\n","    #return the normalized vectors of the sentence\n","    return vectors / np.sqrt((vectors ** 2).sum())"],"metadata":{"id":"STFXxv5DVOkz","executionInfo":{"status":"ok","timestamp":1650842755983,"user_tz":240,"elapsed":29,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def predict_LR(input_text, wordset):\n","  if wordset=='CV':\n","    x_test = CV.transform(input_text)\n","    y_pred = model_CV_LR.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  else:\n","    x_test = TFIDF.transform(input_text)\n","    y_pred = model_TFIDF_LR.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_NB(input_text, wordset):\n","  if wordset=='CV':\n","    x_test = CV.transform(input_text)\n","    y_pred = model_CV_NB.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  else:\n","    x_test = TFIDF.transform(input_text)\n","    y_pred = model_TFIDF_NB.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_XG(input_text, wordset):\n","  if wordset=='CV':\n","    x_test = CV.transform(input_text)\n","    y_pred = model_CV_XG.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  else:\n","    x_test = TFIDF.transform(input_text)\n","    y_pred = model_TFIDF_XG.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_SVM(input_text, wordset):\n","  if wordset=='CV':\n","    x_test = CV.transform(input_text)\n","    x_test = SVD.transform(x_test)\n","    x_test = SVMScaler.transform(x_test)\n","    y_pred = model_CV_SVM.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  else:\n","    x_test = TFIDF.transform(input_text)\n","    x_test = SVD.transform(x_test)\n","    x_test = SVMScaler.transform(x_test)\n","    y_pred = model_TFIDF_SVM.predict(x_test)\n","    y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_glove(input_text):\n","  x_test = []\n","  for sentence in tqdm(input_text):\n","    x_test.append(tokenized_sentence(sentence))\n","  x_test = np.array(x_test)\n","  y_pred = GLOVE_XB.predict(x_test)\n","  y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_vanilla_ann(input_text):\n","  x_test = []\n","  for sentence in tqdm(input_text):\n","    x_test.append(tokenized_sentence(sentence))\n","  x_test = np.array(x_test)\n","  x_test = NNScaler.transform(x_test)\n","  y_pred = vanillann.predict(x_test).argmax(axis=-1)\n","  y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]\n","\n","def predict_biLSTM(input_text):\n","  x_test = token.texts_to_sequences(input_text)\n","  x_test = sequence.pad_sequences(x_test, maxlen=100)\n","  y_pred = biLSTM.predict(x_test).argmax(axis=-1)\n","  y_pred = LE.inverse_transform(y_pred)\n","  return y_pred[0]"],"metadata":{"id":"78SUSj-SUpUa","executionInfo":{"status":"ok","timestamp":1650842756113,"user_tz":240,"elapsed":156,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def prediction(input_text, model, word_set):\n","  input_text = preprocess_text(input_text)\n","  input_text = np.array([input_text])\n","  if model==\"Logistic Regression\":\n","    if word_set=='Count Vectorizer':\n","      return predict_LR(input_text,'CV')\n","    else:\n","      return predict_LR(input_text,'TFIDF')\n","  elif model==\"Naive Bayes\":\n","    if word_set=='Count Vectorizer':\n","      return predict_NB(input_text,'CV')\n","    else:\n","      return predict_NB(input_text,'TFIDF')\n","  elif model==\"XGBoost\":\n","    if word_set=='Count Vectorizer':\n","      return predict_XG(input_text,'CV')\n","    else:\n","      return predict_XG(input_text,'TFIDF')\n","  elif model==\"Support Vector Machine\":\n","    if word_set=='Count Vectorizer':\n","      return predict_SVM(input_text,'CV')\n","    else:\n","      return predict_SVM(input_text,'TFIDF')\n","  elif model==\"Sent2Vec\":\n","    return predict_glove(input_text)\n","  elif model==\"Vanilla ANN\":\n","    return predict_vanilla_ann(input_text)\n","  else:\n","    return predict_biLSTM(input_text)"],"metadata":{"id":"rpQh1CHygAvo","executionInfo":{"status":"ok","timestamp":1650842756114,"user_tz":240,"elapsed":6,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Gradio application\n","app_gradio = gr.Interface(\n","    fn = prediction,\n","    inputs = [\"text\",gr.inputs.Radio([\"Logistic Regression\", \"Naive Bayes\", \"XGBoost\", \"Support Vector Machine\",\"Sent2Vec\",\"Vanilla ANN\", \"Bi-LSTM\"]),gr.inputs.Radio([\"Count Vectorizer\", \"TFIDF\"])],\n","    outputs = \"text\",\n","    title=\"CYBERBULLYING TEXT CLASSIFICATION\",\n","    description=\"As social media usage grows across all age groups, the great majority of individuals rely on this crucial medium for day-to-day communication. Because of the pervasiveness of social media, cyberbullying may affect anybody at any time or from any location, and the internet's relative anonymity makes such personal attacks more difficult to stop than conventional bullying.\"\n",")\n","app_gradio.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"id":"PF5GpW5fb_L7","executionInfo":{"status":"ok","timestamp":1650842815385,"user_tz":240,"elapsed":3507,"user":{"displayName":"Anshul Navinbhai Patel","userId":"03629622706764779716"}},"outputId":"71518078-d6aa-487e-c041-63dcae4ff7eb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n","Running on public URL: https://20260.gradio.app\n","\n","This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.lib.display.IFrame at 0x7fd5a098d790>"],"text/html":["\n","        <iframe\n","            width=\"900\"\n","            height=\"500\"\n","            src=\"https://20260.gradio.app\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(<fastapi.applications.FastAPI at 0x7fd639aba2d0>,\n"," 'http://127.0.0.1:7860/',\n"," 'https://20260.gradio.app')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[""],"metadata":{"id":"WjrvBYyb5jzO"},"execution_count":null,"outputs":[]}]}