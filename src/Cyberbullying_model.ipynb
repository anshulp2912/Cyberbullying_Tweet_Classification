{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cyberbullying_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CYBER BULLYING MODELLING NOTEBOOK\n",
        "\n",
        "---\n",
        "\n",
        "## NCSU CSC 591: Algorithms for Data Guided Buisness Intelligence\n",
        "\n",
        "---\n",
        "As social media usage grows across all age groups, the great majority of individuals rely on this crucial medium for day-to-day communication. Because of the pervasiveness of social media, cyberbullying may affect anybody at any time or from any location, and the internet's relative anonymity makes such personal attacks more difficult to stop than conventional bullying.\n",
        "\n",
        "\n",
        "In light of this, this dataset comprises over 47000 tweets labeled with the following cyberbullying categories: Age, Ethnicity, Gender, Religion, Other sort of cyberbullying, Not cyberbullying.\n",
        "\n",
        "Trigger Warning: These tweets either describe a bullying occurrence or are the crime itself; consequently, read them until you are comfortable.\n",
        "\n",
        "---\n",
        "\n",
        "#### Contributors: Anmolika Goyal(agoyal4), Anshul Navinbhai Patel(apatel28), Shubhangi Jain(sjain29)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsEXdzOqwYDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect the Google Drive"
      ],
      "metadata": {
        "id": "2TYqGFsvyzax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IixDRU-oyy2i",
        "outputId": "7770b079-c167-4fa9-b48a-019dfad5edf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "n_WLcgFWwXO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the libraries\n",
        "!pip install kaggle\n",
        "!pip install emoji==1.6.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiCHLrFqxi3g",
        "outputId": "7539a2a5-38d5-46da-ce3a-24eb426d4b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting emoji==1.6.3\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=fc060435f3d56c9937c23d71d10d51b818723f22bb8e79e5042fc947ab52aa98\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the kaggle.json\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "! mkdir ~/.kaggle\n",
        "!cp \"/content/gdrive/MyDrive/Github Repos/kaggle.json\" ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YaQZs4QQyorH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVMiflU8wDFI",
        "outputId": "5fd93de1-3801-45a0-9078-5d49e5add4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# General Librarires\n",
        "import kaggle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re, string\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "# Model Saving\n",
        "import joblib\n",
        "import pickle\n",
        "# Scikit-Learn Functions\n",
        "from sklearn import preprocessing, decomposition, metrics, pipeline\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Machine Learning\n",
        "import xgboost as xgb\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords') \n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset from kaggle\n",
        "\n",
        "Dataset Link: https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification"
      ],
      "metadata": {
        "id": "vJ6nDNEcxJgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/cyberbullying-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFmAb-JWxJCZ",
        "outputId": "508f8ab6-2b48-4f94-de2c-f18576cd0167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cyberbullying-classification.zip to /content\n",
            "\r  0% 0.00/2.82M [00:00<?, ?B/s]\n",
            "\r100% 2.82M/2.82M [00:00<00:00, 78.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/cyberbullying-classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n94fXAYcz_zg",
        "outputId": "a40bad66-f0f8-4439-bbe8-59e3f9aa6c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cyberbullying-classification.zip\n",
            "  inflating: cyberbullying_tweets.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from csv\n",
        "df = pd.read_csv('/content/cyberbullying_tweets.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pd_IEMs-1BVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "96aea700-75b6-4d2a-c102-e002c5b38b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8378d9c1-356c-4f0c-ae6a-8fed103b185a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8378d9c1-356c-4f0c-ae6a-8fed103b185a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8378d9c1-356c-4f0c-ae6a-8fed103b185a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8378d9c1-356c-4f0c-ae6a-8fed103b185a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "kcSeNKU91ulv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the information gathered from Exploratory Data Analysis Notebook: [Link](https://github.com/anshulp2912/Cyberbullying_Tweet_Classification/blob/main/src/cyberbullying_EDA.ipynb)"
      ],
      "metadata": {
        "id": "KEFmHHwt2H6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the duplicate rows from dataset\n",
        "dataset = df.drop_duplicates()\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "GrvZURWD1YK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8dff28ec-3b39-4897-a704-3bd23c7018fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f58ee38-fe5f-4b97-abd1-71ca69f3f6c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f58ee38-fe5f-4b97-abd1-71ca69f3f6c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f58ee38-fe5f-4b97-abd1-71ca69f3f6c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f58ee38-fe5f-4b97-abd1-71ca69f3f6c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing functions\n",
        "\n",
        "# Remove emojis from text\n",
        "def remove_emoji(txt):\n",
        "  txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n",
        "  return txt\n",
        "\n",
        "# Expand common abbreviations\n",
        "def expand_txt(txt):\n",
        "  txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "  txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "  txt = re.sub(r\"can\\'t\", \"can not\", txt)\n",
        "  txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "  txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "  txt = re.sub(r\"\\'s\", \" is\", txt)\n",
        "  txt = re.sub(r\"\\'m\", \" am\", txt)\n",
        "  txt = re.sub(r\"n\\'t\", \" not\", txt)\n",
        "  txt = re.sub(r\"\\'t\", \" not\", txt)\n",
        "  return txt\n",
        "\n",
        "# Remove characters, links, mentions, and punctuations\n",
        "def clean_nonwanted_chars(txt):\n",
        "  # Remove characters\n",
        "  txt = txt.replace('\\n', ' ')\n",
        "  txt = txt.replace('\\r', '')\n",
        "  # Remove mentions and links\n",
        "  txt = re.sub(r'[^\\x00-\\x7f]',r'', txt)\n",
        "  # Remove punctuations\n",
        "  punc_remove = string.punctuation\n",
        "  punc_list = str.maketrans('', '', punc_remove)\n",
        "  txt = txt.translate(punc_list)\n",
        "  txt = [word for word in txt.split() if word not in stop_words]\n",
        "  txt = ' '.join(txt)\n",
        "  return txt\n",
        "\n",
        "# Remove Hashtags\n",
        "def remove_hash(txt):\n",
        "  txt = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', txt)) \n",
        "  txt = \" \".join(word.strip() for word in re.split('#|_', txt))\n",
        "  return txt\n",
        "\n",
        "# Remove characters from between the words\n",
        "def remove_chars(txt):\n",
        "    clean = []\n",
        "    for word in txt.split(' '):\n",
        "        if ('&' in word) | ('$' in word):\n",
        "            clean.append('')\n",
        "        else:\n",
        "            clean.append(word)\n",
        "    txt = ' '.join(clean)\n",
        "    return txt\n",
        "\n",
        "# Remove multiple spaces and tabs\n",
        "def remove_space(txt):\n",
        "  txt = re.sub(\"\\s\\s+\" , \" \", txt)\n",
        "  return txt"
      ],
      "metadata": {
        "id": "uTbVL1whMTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the textual data\n",
        "def preprocess_text(txt):\n",
        "  txt = txt.lower()\n",
        "  txt = remove_emoji(txt)\n",
        "  txt = expand_txt(txt)\n",
        "  txt = clean_nonwanted_chars(txt)\n",
        "  txt = remove_hash(txt)\n",
        "  txt = remove_chars(txt)\n",
        "  txt = remove_space(txt)\n",
        "  # Stemming the text\n",
        "  tokens = nltk.word_tokenize(txt)\n",
        "  PS = nltk.stem.PorterStemmer()\n",
        "  txt = ' '.join([PS.stem(words) for words in tokens])\n",
        "  return txt\n",
        "\n",
        "# Generate clean text\n",
        "clean_txt = []\n",
        "for txt in list(dataset.tweet_text.values):\n",
        "  clean_txt.append(preprocess_text(txt))\n",
        "\n",
        "# Replace text in dataframe\n",
        "dataset['tweet_text'] = clean_txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVw883IpGmPN",
        "outputId": "394ed6c5-7589-4a88-e3dc-45b95d2621d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the duplicate rows from dataset again after cleaning\n",
        "dataset = dataset.drop_duplicates()\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "w53DKLuxVaAf",
        "outputId": "872cbb4f-9761-4aff-b6be-eb1b765478ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0                 word katandandr food crapilici mkr  not_cyberbullying\n",
              "1  aussietv white mkr theblock imacelebrityau tod...  not_cyberbullying\n",
              "2       xochitlsuckkk classi whore red velvet cupcak  not_cyberbullying\n",
              "3  jasongio meh p thank head concern anoth angri ...  not_cyberbullying\n",
              "4  rudhoeenglish isi account pretend kurdish acco...  not_cyberbullying"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acc229df-19bb-4e89-8abf-34dcf50476c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>word katandandr food crapilici mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xochitlsuckkk classi whore red velvet cupcak</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jasongio meh p thank head concern anoth angri ...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rudhoeenglish isi account pretend kurdish acco...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acc229df-19bb-4e89-8abf-34dcf50476c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-acc229df-19bb-4e89-8abf-34dcf50476c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-acc229df-19bb-4e89-8abf-34dcf50476c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the X and y dataset\n",
        "X = dataset.tweet_text.values\n",
        "y = dataset.cyberbullying_type.values"
      ],
      "metadata": {
        "id": "bjXTmLDiVULV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the textual labels to numericals value\n",
        "LE = preprocessing.LabelEncoder()\n",
        "y = LE.fit_transform(y)\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Label Encoder...')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/LE.pkl', 'wb') as f:\n",
        "    pickle.dump(LE, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuJMn94V-QXP",
        "outputId": "6d3175ae-8574-4195-d7ea-e21437568ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving Label Encoder...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Dataset into Train(70%), Validation(15%), Test(15%)\n",
        "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, stratify=y, test_size=0.3, shuffle=True, random_state=111)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, stratify=y_remaining, test_size=0.5, shuffle=True, random_state=111)"
      ],
      "metadata": {
        "id": "7Pn67aLfBv3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate word vectors from sentences\n",
        "\n",
        "\n",
        "*   Count Vectorizer\n",
        "*   TFIDF Vectorizer\n",
        "\n"
      ],
      "metadata": {
        "id": "Dyw6eMVLWFP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Vectorizer\n",
        "CV = CountVectorizer(analyzer='word',max_features=3000,token_pattern=r'\\w{1,}',ngram_range=(1, 3), stop_words = 'english')\n",
        "CV.fit(list(X_train)+list(X_val)+list(X_test))\n",
        "X_train_CV = CV.transform(X_train)\n",
        "X_val_CV = CV.transform(X_val)\n",
        "X_test_CV = CV.transform(X_test)\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Count Vectorizer...')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/CountVectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(CV, f)"
      ],
      "metadata": {
        "id": "_coqhaI9W26u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8e88ae-8717-4d8a-b9d3-a3782c89671a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving Count Vectorizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF Vectorizer\n",
        "TFIDF = TfidfVectorizer(min_df=3,  max_features=3000, strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
        "TFIDF.fit(list(X_train)+list(X_val)+list(X_test))\n",
        "X_train_TFIDF = TFIDF.transform(X_train)\n",
        "X_val_TFIDF = TFIDF.transform(X_val)\n",
        "X_test_TFIDF = TFIDF.transform(X_test)\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving TFIDF Vectorizer...')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/TFIDFVectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(TFIDF, f)"
      ],
      "metadata": {
        "id": "AXwUOmY2Xv_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3af555-49e9-4434-8e9c-85ee884588d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving TFIDF Vectorizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "S2CpfFSNVIb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Machine Learning Models\n",
        "\n",
        "\n",
        "*   Logistic Regression\n",
        "*   Naive Bayes\n",
        "*   XGBoost\n",
        "*   Support Vector Machines\n"
      ],
      "metadata": {
        "id": "AvMu9MsZZ8d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "print('LOGISTIC REGRESSION MODEL')\n",
        "\n",
        "# Count Vectorizer\n",
        "print('\\nCount Vectorizer Model')\n",
        "model_CV_LR = LogisticRegression(solver='saga')\n",
        "model_CV_LR.fit(X_train_CV, y_train)\n",
        "y_pred_val_CV = model_CV_LR.predict_proba(X_val_CV)\n",
        "y_pred_test_CV = model_CV_LR.predict_proba(X_test_CV)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_CV_LR, X_train_CV, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('CV Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('CV Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_CV))\n",
        "print('CV Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_CV))\n",
        "\n",
        "# TFIDF Vectorizer\n",
        "print('\\nTFIDF Vectorizer Model')\n",
        "model_TFIDF_LR = LogisticRegression(solver='saga')\n",
        "model_TFIDF_LR.fit(X_train_TFIDF, y_train)\n",
        "y_pred_val_TFIDF = model_TFIDF_LR.predict_proba(X_val_TFIDF)\n",
        "y_pred_test_TFIDF = model_TFIDF_LR.predict_proba(X_test_TFIDF)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_TFIDF_LR, X_train_TFIDF, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('TFIDF Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('TFIDF Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_TFIDF))\n",
        "print('TFIDF Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_TFIDF))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(model_CV_LR, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_LR.sav')\n",
        "joblib.dump(model_TFIDF_LR, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_LR.sav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htBKrXl7Gjby",
        "outputId": "27b52df8-5c66-45fb-ba08-5908ec4edc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOGISTIC REGRESSION MODEL\n",
            "\n",
            "Count Vectorizer Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV Cross Validation Train Accuracy: 0.826\n",
            "CV Validation Loss: 0.419\n",
            "CV Test Loss: 0.416\n",
            "\n",
            "TFIDF Vectorizer Model\n",
            "TFIDF Cross Validation Train Accuracy: 0.822\n",
            "TFIDF Validation Loss: 0.448\n",
            "TFIDF Test Loss: 0.451\n",
            "\n",
            "Saving Model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_LR.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "print('NAIVE BAYES MODEL')\n",
        "\n",
        "# Count Vectorizer\n",
        "print('\\nCount Vectorizer Model')\n",
        "model_CV_NB = MultinomialNB()\n",
        "model_CV_NB.fit(X_train_CV, y_train)\n",
        "y_pred_val_CV = model_CV_NB.predict_proba(X_val_CV)\n",
        "y_pred_test_CV = model_CV_NB.predict_proba(X_test_CV)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_CV_NB, X_train_CV, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('CV Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('CV Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_CV))\n",
        "print('CV Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_CV))\n",
        "\n",
        "# TFIDF Vectorizer\n",
        "print('\\nTFIDF Vectorizer Model')\n",
        "model_TFIDF_NB = MultinomialNB()\n",
        "model_TFIDF_NB.fit(X_train_TFIDF, y_train)\n",
        "y_pred_val_TFIDF = model_TFIDF_NB.predict_proba(X_val_TFIDF)\n",
        "y_pred_test_TFIDF = model_TFIDF_NB.predict_proba(X_test_TFIDF)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_TFIDF_NB, X_train_TFIDF, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('TFIDF Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('TFIDF Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_TFIDF))\n",
        "print('TFIDF Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_TFIDF))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(model_CV_NB, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_NB.sav')\n",
        "joblib.dump(model_TFIDF_NB, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_NB.sav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uftm_NTcjVxS",
        "outputId": "a3c7dc67-1b63-4153-e1a2-7e3a3f535ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAIVE BAYES MODEL\n",
            "\n",
            "Count Vectorizer Model\n",
            "CV Cross Validation Train Accuracy: 0.770\n",
            "CV Validation Loss: 0.686\n",
            "CV Test Loss: 0.707\n",
            "\n",
            "TFIDF Vectorizer Model\n",
            "TFIDF Cross Validation Train Accuracy: 0.767\n",
            "TFIDF Validation Loss: 0.632\n",
            "TFIDF Test Loss: 0.635\n",
            "\n",
            "Saving Model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_NB.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST\n",
        "print('XGBOOST MODEL')\n",
        "\n",
        "# Count Vectorizer\n",
        "print('\\nCount Vectorizer Model')\n",
        "model_CV_XG = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "model_CV_XG.fit(X_train_CV.tocsc(), y_train)\n",
        "y_pred_val_CV = model_CV_XG.predict_proba(X_val_CV.tocsc())\n",
        "y_pred_test_CV = model_CV_XG.predict_proba(X_test_CV.tocsc())\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_CV_XG, X_train_CV.tocsc(), y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('CV Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('CV Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_CV))\n",
        "print('CV Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_CV))\n",
        "\n",
        "# TFIDF Vectorizer\n",
        "print('\\nTFIDF Vectorizer Model')\n",
        "model_TFIDF_XG = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "model_TFIDF_XG.fit(X_train_TFIDF.tocsc(), y_train)\n",
        "y_pred_val_TFIDF = model_TFIDF_XG.predict_proba(X_val_TFIDF.tocsc())\n",
        "y_pred_test_TFIDF = model_TFIDF_XG.predict_proba(X_test_TFIDF.tocsc())\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_TFIDF_XG, X_train_TFIDF.tocsc(), y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('TFIDF Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('TFIDF Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_TFIDF))\n",
        "print('TFIDF Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_TFIDF))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(model_CV_XG, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_XG.sav')\n",
        "joblib.dump(model_TFIDF_XG, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_XG.sav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRC-Byz4hzNZ",
        "outputId": "663b412e-f361-4986-9337-b33adc8bf473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBOOST MODEL\n",
            "\n",
            "Count Vectorizer Model\n",
            "CV Cross Validation Train Accuracy: 0.834\n",
            "CV Validation Loss: 0.398\n",
            "CV Test Loss: 0.390\n",
            "\n",
            "TFIDF Vectorizer Model\n",
            "TFIDF Cross Validation Train Accuracy: 0.831\n",
            "TFIDF Validation Loss: 0.403\n",
            "TFIDF Test Loss: 0.399\n",
            "\n",
            "Saving Model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_XG.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machines\n",
        "print('SVM MODEL')\n",
        "\n",
        "# Reduce the size of training set to speed up training\n",
        "SVD = decomposition.TruncatedSVD(n_components=150)\n",
        "X_train_CV_svd = SVD.fit_transform(X_train_CV)\n",
        "X_val_CV_svd = SVD.transform(X_val_CV)\n",
        "X_test_CV_svd = SVD.transform(X_test_CV)\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_CV_svd = scaler.fit_transform(X_train_CV_svd)\n",
        "X_val_CV_svd = scaler.transform(X_val_CV_svd)\n",
        "X_test_CV_svd = scaler.transform(X_test_CV_svd)\n",
        "\n",
        "# Count Vectorizer\n",
        "print('\\nCount Vectorizer Model')\n",
        "model_CV_SVM = SVC(C=1.0, probability=True)\n",
        "model_CV_SVM.fit(X_train_CV_svd, y_train)\n",
        "y_pred_val_CV = model_CV_SVM.predict_proba(X_val_CV_svd)\n",
        "y_pred_test_CV = model_CV_SVM.predict_proba(X_test_CV_svd)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_CV_SVM, X_train_CV_svd, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('CV Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('CV Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_CV))\n",
        "print('CV Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_CV))\n",
        "\n",
        "# Reduce the size of training set to speed up training\n",
        "SVD = decomposition.TruncatedSVD(n_components=150)\n",
        "X_train_TFIDF_svd = SVD.fit_transform(X_train_TFIDF)\n",
        "X_val_TFIDF_svd = SVD.transform(X_val_TFIDF)\n",
        "X_test_TFIDF_svd = SVD.transform(X_test_TFIDF)\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_TFIDF_svd = scaler.fit_transform(X_train_TFIDF_svd)\n",
        "X_val_TFIDF_svd = scaler.transform(X_val_TFIDF_svd)\n",
        "X_test_TFIDF_svd = scaler.transform(X_test_TFIDF_svd)\n",
        "\n",
        "# TFIDF Vectorizer\n",
        "print('\\nTFIDF Vectorizer Model')\n",
        "model_TFIDF_SVM = SVC(C=1.0, probability=True)\n",
        "model_TFIDF_SVM.fit(X_train_TFIDF_svd, y_train)\n",
        "y_pred_val_TFIDF = model_TFIDF_SVM.predict_proba(X_val_TFIDF_svd)\n",
        "y_pred_test_TFIDF = model_TFIDF_SVM.predict_proba(X_test_TFIDF_svd)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(model_TFIDF_SVM, X_train_TFIDF_svd, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "print('TFIDF Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('TFIDF Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_TFIDF))\n",
        "print('TFIDF Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_TFIDF))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(model_CV_SVM, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_CV_SVM.sav')\n",
        "joblib.dump(model_TFIDF_SVM, '/content/gdrive/Shareddrives/ADBI_Capstone/models/model_TFIDF_SVM.sav')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/SVD.pkl', 'wb') as f:\n",
        "    pickle.dump(SVD, f)\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/SVMScaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZhM0uHxpn3b",
        "outputId": "568a875a-c682-465d-9884-ba448553d2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM MODEL\n",
            "\n",
            "Count Vectorizer Model\n",
            "CV Cross Validation Train Accuracy: 0.795\n",
            "CV Validation Loss: 0.514\n",
            "CV Test Loss: 0.503\n",
            "\n",
            "TFIDF Vectorizer Model\n",
            "TFIDF Cross Validation Train Accuracy: 0.808\n",
            "TFIDF Validation Loss: 0.481\n",
            "TFIDF Test Loss: 0.468\n",
            "\n",
            "Saving Model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word Vectors\n"
      ],
      "metadata": {
        "id": "1Z-204u0tv4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Glove Vectors\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g64FeoT_t_VC",
        "outputId": "6200e7e5-e5ea-4d9f-b5e3-d26a83c587e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-23 21:51:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-04-23 21:51:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 42s  \n",
            "\n",
            "2022-04-23 21:54:42 (5.07 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the contents of the glove folder\n",
        "!unzip /content/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6w-YFLVibim",
        "outputId": "11ae3f5b-7bd8-4a86-9b81-b28228ba2d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary and load all the glove vectors into it.\n",
        "# This dictionary will be used to fetch the values in the normalized vectors created for the sentences\n",
        "embeddings = {}\n",
        "with open(\"/content/glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
        "    #go line by line and map the tokens with the vectors in the dictionary\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        token = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings[token] = vector\n",
        "\n",
        "print(str(len(embeddings))+' word vectors have been found in this dictionary')\n",
        "\n",
        "# Saving the embeddings\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTry0zzPmLxr",
        "outputId": "4bfc7fad-cce6-4487-e219-a6f176da0294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000 word vectors have been found in this dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using word_tokenize to create vectors which are normalized for the wole sentence\n",
        "def tokenized_sentence(s):\n",
        "    text = str(s).lower()\n",
        "    #use word_tokenize to split the text into words\n",
        "    text = word_tokenize(text)\n",
        "    #create a list named text which stores the words that are not in stop_words\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    #check if the word is an alphanumeric\n",
        "    text = [word for word in text if word.isalpha()]\n",
        "    values = []\n",
        "    #for each word in text, append the value of the vector for that word into the values list\n",
        "    for word in text:\n",
        "        try:\n",
        "            values.append(embeddings[word])\n",
        "        except:\n",
        "            continue\n",
        "    values = np.array(values)\n",
        "    vectors = values.sum(axis=0)\n",
        "    if(type(vectors) != np.ndarray):\n",
        "        return np.zeros(300)\n",
        "    #return the normalized vectors of the sentence\n",
        "    return vectors / np.sqrt((vectors ** 2).sum())"
      ],
      "metadata": {
        "id": "jCijNiqHmqYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the data into training set, testing set and validation set\n",
        "# use tokenize_sentence function to generate the vectors for each statement in the respective set.\n",
        "\n",
        "X_training = []\n",
        "#for each sentence in the X_train set, create normalized vectors and append it to the X_training list\n",
        "for sentence in tqdm(X_train):\n",
        "  X_training.append(tokenized_sentence(sentence))\n",
        "#Convert this list into an array using np.array\n",
        "X_training = np.array(X_training)\n",
        "\n",
        "X_validation = []\n",
        "#for each sentence in the X_val set, create normalized vectors and append it to the X_validation list\n",
        "for sentence in tqdm(X_val):\n",
        "  X_validation.append(tokenized_sentence(sentence))\n",
        "#Convert this list into an array using np.array\n",
        "X_validation = np.array(X_validation)\n",
        "\n",
        "X_testing = []\n",
        "#for each sentence in the X_test set, create normalized vectors and append it to the X_testing list\n",
        "for sentence in tqdm(X_test):\n",
        "  X_testing.append(tokenized_sentence(sentence))\n",
        "#Convert this list into an array using np.array\n",
        "X_testing = np.array(X_testing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk0oTKWspgcq",
        "outputId": "929e7a7f-e0d3-4d1a-8590-f3787983b576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33185/33185 [00:12<00:00, 2662.78it/s]\n",
            "100%|██████████| 7111/7111 [00:01<00:00, 3942.58it/s]\n",
            "100%|██████████| 7112/7112 [00:01<00:00, 4237.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on glove vectors using XGBOOST\n",
        "print('GLOVE XGBOOST')\n",
        "GLOVE_XB = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "#fit the model \n",
        "GLOVE_XB.fit(X_training, y_train)\n",
        "#use predict_proba() which returns an array of lists containing the class probabilities for the input data points\n",
        "y_pred_val_GLOVE = GLOVE_XB.predict_proba(X_validation)\n",
        "y_pred_test_GLOVE = GLOVE_XB.predict_proba(X_testing)\n",
        "# Cross-Validation Score\n",
        "scores = cross_val_score(GLOVE_XB, X_training, y_train, cv=5)\n",
        "scores = pd.Series(scores)\n",
        "# Calculate the validation accuracy\n",
        "print('GLOVE Cross Validation Train Accuracy: %0.3f'%scores.mean())\n",
        "# Calculate Multinomial Log loss \n",
        "print('GLOVE Validation Loss: %0.3f'%metrics.log_loss(y_val, y_pred_val_GLOVE))\n",
        "print('GLOVE Test Loss: %0.3f'%metrics.log_loss(y_test, y_pred_test_GLOVE))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(GLOVE_XB, '/content/gdrive/Shareddrives/ADBI_Capstone/models/GLOVE_XB.sav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5bW_pq6rHw8",
        "outputId": "c568bbea-d18e-4e41-900f-78d0af639a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLOVE XGBOOST\n",
            "GLOVE Cross Validation Train Accuracy: 0.781\n",
            "GLOVE Validation Loss: 0.554\n",
            "GLOVE Test Loss: 0.560\n",
            "\n",
            "Saving Model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/Shareddrives/ADBI_Capstone/models/GLOVE_XB.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Neural Network\n",
        "\n",
        "\n",
        "*   Vanilla Neural Network\n",
        "*   Bidirectional LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "5j9Wc5CMsvKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing the Glove vectors for faster execution\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_training_SCL = scaler.fit_transform(X_training)\n",
        "X_validation_SCL = scaler.transform(X_validation)\n",
        "X_testing_SCL = scaler.transform(X_testing)\n",
        "\n",
        "# Use one-hot encoding to convert target to binary\n",
        "y_train_enc = np_utils.to_categorical(y_train)\n",
        "y_val_enc = np_utils.to_categorical(y_val)\n",
        "y_test_enc = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Saving the variables\n",
        "print('\\nSaving the variables...')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/NNScaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "IX2lVj_Estwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a57d15-7907-49d0-df4b-607a5cfdaacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving the variables...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vanilla Neural Network\n",
        "print('Vanilla Neural Network')\n",
        "\n",
        "# Define the network\n",
        "# using the sequetial functionality of neural networks in python, create a neural network\n",
        "# In sequential neural network, the input of the current layer is the output of the previous layer\n",
        "vanillann = Sequential()\n",
        "vanillann.add(Dense(300, input_dim=300, activation='relu'))\n",
        "vanillann.add(Dropout(0.2))\n",
        "vanillann.add(BatchNormalization())\n",
        "vanillann.add(Dense(200, activation='relu'))\n",
        "vanillann.add(Dropout(0.2))\n",
        "vanillann.add(BatchNormalization())\n",
        "vanillann.add(Dense(300, activation='relu'))\n",
        "vanillann.add(Dropout(0.3))\n",
        "vanillann.add(BatchNormalization())\n",
        "vanillann.add(Dense(6))\n",
        "vanillann.add(Activation('softmax'))\n",
        "\n",
        "# Use compile function to compile the model and give a summary of the data in the model\n",
        "vanillann.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "vanillann.summary()\n",
        "\n",
        "# Initialize the H=hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Fitting the model\n",
        "history = vanillann.fit(X_training_SCL, y_train_enc, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_validation_SCL, y_val_enc))\n",
        "\n",
        "# Predict on Validation and Test Set\n",
        "# The predict funtion helps predict the labels of the data values on the basis of the model which is trained\n",
        "y_pred_val_ANN = vanillann.predict(X_validation_SCL)\n",
        "y_pred_test_ANN = vanillann.predict(X_testing_SCL)\n",
        "\n",
        "#Display the accuracy of the Vanilla ANN model that we trained\n",
        "print('Vanilla ANN Train Accuracy: %0.3f'%history.history['accuracy'][-1])\n",
        "# Calculate Multinomial Log loss \n",
        "print('Vanilla ANN Validation Loss: %0.3f'%metrics.log_loss(y_val_enc, y_pred_val_ANN))\n",
        "print('Vanilla ANN Test Loss: %0.3f'%metrics.log_loss(y_test_enc, y_pred_test_ANN))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(vanillann, '/content/gdrive/Shareddrives/ADBI_Capstone/models/vanillann.sav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rqb_iyvvGEp",
        "outputId": "4fc2bd34-0313-4741-e5aa-54a65ed6327c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla Neural Network\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 300)               90300     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 300)              1200      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               60200     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 300)               60300     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 1806      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215,806\n",
            "Trainable params: 214,206\n",
            "Non-trainable params: 1,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "519/519 [==============================] - 8s 7ms/step - loss: 0.6937 - accuracy: 0.7358 - val_loss: 0.5059 - val_accuracy: 0.8030\n",
            "Epoch 2/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.5198 - accuracy: 0.7916 - val_loss: 0.4786 - val_accuracy: 0.8051\n",
            "Epoch 3/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.4769 - accuracy: 0.8047 - val_loss: 0.4731 - val_accuracy: 0.8107\n",
            "Epoch 4/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.4563 - accuracy: 0.8145 - val_loss: 0.4675 - val_accuracy: 0.8165\n",
            "Epoch 5/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.4372 - accuracy: 0.8207 - val_loss: 0.4786 - val_accuracy: 0.8111\n",
            "Epoch 6/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.4139 - accuracy: 0.8261 - val_loss: 0.4674 - val_accuracy: 0.8151\n",
            "Epoch 7/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.4060 - accuracy: 0.8321 - val_loss: 0.4766 - val_accuracy: 0.8087\n",
            "Epoch 8/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3885 - accuracy: 0.8397 - val_loss: 0.4757 - val_accuracy: 0.8148\n",
            "Epoch 9/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3744 - accuracy: 0.8440 - val_loss: 0.4765 - val_accuracy: 0.8102\n",
            "Epoch 10/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3665 - accuracy: 0.8454 - val_loss: 0.4830 - val_accuracy: 0.8163\n",
            "Epoch 11/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3531 - accuracy: 0.8519 - val_loss: 0.4836 - val_accuracy: 0.8134\n",
            "Epoch 12/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3440 - accuracy: 0.8561 - val_loss: 0.4919 - val_accuracy: 0.8061\n",
            "Epoch 13/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3333 - accuracy: 0.8620 - val_loss: 0.5033 - val_accuracy: 0.8104\n",
            "Epoch 14/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3226 - accuracy: 0.8675 - val_loss: 0.5124 - val_accuracy: 0.8052\n",
            "Epoch 15/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3169 - accuracy: 0.8682 - val_loss: 0.5120 - val_accuracy: 0.8079\n",
            "Epoch 16/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.3054 - accuracy: 0.8726 - val_loss: 0.5135 - val_accuracy: 0.8090\n",
            "Epoch 17/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.2967 - accuracy: 0.8763 - val_loss: 0.5294 - val_accuracy: 0.8085\n",
            "Epoch 18/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.2940 - accuracy: 0.8760 - val_loss: 0.5386 - val_accuracy: 0.8055\n",
            "Epoch 19/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.2824 - accuracy: 0.8812 - val_loss: 0.5400 - val_accuracy: 0.8092\n",
            "Epoch 20/20\n",
            "519/519 [==============================] - 3s 6ms/step - loss: 0.2806 - accuracy: 0.8830 - val_loss: 0.5511 - val_accuracy: 0.8048\n",
            "Vanilla ANN Train Accuracy: 0.883\n",
            "Vanilla ANN Validation Loss: 0.551\n",
            "Vanilla ANN Test Loss: 0.531\n",
            "\n",
            "Saving Model...\n",
            "INFO:tensorflow:Assets written to: ram://f6e7a2a6-0470-4761-995d-17460cd55cd5/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/Shareddrives/ADBI_Capstone/models/vanillann.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bi-directional LSTM Model\n",
        "print('Bi-directional LSTM Model')\n",
        "\n",
        "# Generate sequences for LSTM\n",
        "#use texts_to sequences to convert each text in data to a sequence of integers\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 100\n",
        "token.fit_on_texts(list(X_train) + list(X_val) + list(X_test))\n",
        "X_train_seq = token.texts_to_sequences(X_train)\n",
        "X_val_seq = token.texts_to_sequences(X_val)\n",
        "X_test_seq = token.texts_to_sequences(X_test)\n",
        "\n",
        "# Adding padding to sequences\n",
        "# Use pad_sequences to add padding t each sentence in the Train, Validation and Testing data\n",
        "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_pad = sequence.pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# create an matrix which contains the values and the vectors for the words we have in the dataset\n",
        "word_index = token.word_index\n",
        "vector_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, val in tqdm(word_index.items()):\n",
        "    em_vector = embeddings.get(word)\n",
        "    if em_vector is not None:\n",
        "        vector_matrix[val] = em_vector\n",
        "\n",
        "# Define the network\n",
        "# using the sequetial functionality of neural networks in python, create a neural network\n",
        "# In sequential neural network, the input of the current layer is the output of the previous layer\n",
        "biLSTM = Sequential()\n",
        "biLSTM.add(Embedding(len(word_index) + 1, 300, weights=[vector_matrix], input_length=max_len, trainable=False))\n",
        "biLSTM.add(SpatialDropout1D(0.3))\n",
        "biLSTM.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
        "biLSTM.add(Dense(1024, activation='relu'))\n",
        "biLSTM.add(Dropout(0.8))\n",
        "biLSTM.add(Dense(1024, activation='relu'))\n",
        "biLSTM.add(Dropout(0.8))\n",
        "biLSTM.add(Dense(6))\n",
        "biLSTM.add(Activation('softmax'))\n",
        "\n",
        "# Use compile function to compile the model and give a summary of the data in the model\n",
        "biLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "biLSTM.summary()\n",
        "\n",
        "# Initialize the H=hyperparameters\n",
        "batch_size = 512\n",
        "epochs = 20\n",
        "\n",
        "#Use EarlyStopping function to stop early when the metric stops improving after a few epochs\n",
        "#Fit the model with the early stopping callback\n",
        "stop_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "biLSTM.fit(X_train_pad, y=y_train_enc, batch_size=batch_size, epochs=epochs, \n",
        "          verbose=1, validation_data=(X_val_pad, y_val_enc), callbacks=[stop_callback])\n",
        "\n",
        "# Predict on Validation and Test Set\n",
        "# The predict funtion helps predict the labels of the data values on the basis of the model which is trained\n",
        "y_pred_val_biLSTM = biLSTM.predict(X_val_pad)\n",
        "y_pred_test_biLSTM = biLSTM.predict(X_test_pad)\n",
        "\n",
        "#Display the accuracy of the Bidirectional LSTM model that we trained\n",
        "print('Bidirectional LSTM Train Accuracy: %0.3f'%history.history['accuracy'][-1])\n",
        "# Calculate Multinomial Log loss \n",
        "print('Bidirectional LSTM Validation Loss: %0.3f'%metrics.log_loss(y_val_enc, y_pred_val_biLSTM))\n",
        "print('Bidirectional LSTM Test Loss: %0.3f'%metrics.log_loss(y_test_enc, y_pred_test_biLSTM))\n",
        "\n",
        "# Save the models\n",
        "print('\\nSaving Model...')\n",
        "joblib.dump(biLSTM, '/content/gdrive/Shareddrives/ADBI_Capstone/models/biLSTM.sav')\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/token.pkl', 'wb') as f:\n",
        "    pickle.dump(token, f)\n",
        "with open('/content/gdrive/Shareddrives/ADBI_Capstone/models/word_index.pkl', 'wb') as f:\n",
        "    pickle.dump(word_index, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19q1KL6ouV7b",
        "outputId": "5de4cdc3-2962-4fe7-f0a9-9a68fd99d6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-directional LSTM Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55047/55047 [00:00<00:00, 740021.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 300)          16514400  \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 100, 300)         0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 600)              1442400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1024)              615424    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 6)                 6150      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,627,974\n",
            "Trainable params: 3,113,574\n",
            "Non-trainable params: 16,514,400\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "65/65 [==============================] - 84s 1s/step - loss: 1.0193 - accuracy: 0.6007 - val_loss: 0.5528 - val_accuracy: 0.7779\n",
            "Epoch 2/20\n",
            "65/65 [==============================] - 75s 1s/step - loss: 0.5899 - accuracy: 0.7599 - val_loss: 0.4946 - val_accuracy: 0.7964\n",
            "Epoch 3/20\n",
            "65/65 [==============================] - 77s 1s/step - loss: 0.5428 - accuracy: 0.7795 - val_loss: 0.4735 - val_accuracy: 0.8026\n",
            "Epoch 4/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.5158 - accuracy: 0.7876 - val_loss: 0.4683 - val_accuracy: 0.8041\n",
            "Epoch 5/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.4974 - accuracy: 0.7961 - val_loss: 0.4612 - val_accuracy: 0.8090\n",
            "Epoch 6/20\n",
            "65/65 [==============================] - 77s 1s/step - loss: 0.4785 - accuracy: 0.8024 - val_loss: 0.4460 - val_accuracy: 0.8154\n",
            "Epoch 7/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.4693 - accuracy: 0.8067 - val_loss: 0.4403 - val_accuracy: 0.8147\n",
            "Epoch 8/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.4530 - accuracy: 0.8128 - val_loss: 0.4354 - val_accuracy: 0.8208\n",
            "Epoch 9/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.4404 - accuracy: 0.8185 - val_loss: 0.4401 - val_accuracy: 0.8197\n",
            "Epoch 10/20\n",
            "65/65 [==============================] - 78s 1s/step - loss: 0.4338 - accuracy: 0.8213 - val_loss: 0.4280 - val_accuracy: 0.8238\n",
            "Epoch 11/20\n",
            "65/65 [==============================] - 77s 1s/step - loss: 0.4187 - accuracy: 0.8240 - val_loss: 0.4334 - val_accuracy: 0.8239\n",
            "Epoch 12/20\n",
            "65/65 [==============================] - 77s 1s/step - loss: 0.4159 - accuracy: 0.8264 - val_loss: 0.4279 - val_accuracy: 0.8259\n",
            "Epoch 13/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.4047 - accuracy: 0.8285 - val_loss: 0.4170 - val_accuracy: 0.8280\n",
            "Epoch 14/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.3974 - accuracy: 0.8338 - val_loss: 0.4171 - val_accuracy: 0.8258\n",
            "Epoch 15/20\n",
            "65/65 [==============================] - 76s 1s/step - loss: 0.3914 - accuracy: 0.8361 - val_loss: 0.4195 - val_accuracy: 0.8276\n",
            "Epoch 16/20\n",
            "65/65 [==============================] - 78s 1s/step - loss: 0.3792 - accuracy: 0.8403 - val_loss: 0.4192 - val_accuracy: 0.8269\n",
            "Bidirectional LSTM Train Accuracy: 0.883\n",
            "Bidirectional LSTM Validation Loss: 0.419\n",
            "Bidirectional LSTM Test Loss: 0.414\n",
            "\n",
            "Saving Model...\n",
            "INFO:tensorflow:Assets written to: ram://e45d565e-f431-44cb-bdf3-8dcdc6c3cd3d/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x7f504a319cd0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x7f504a1ef290> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f504a1efa90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5049d50290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DOArqlbD1W_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}